services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ../data/ollama_data:/root/.ollama
      - ../config/ollama-init.sh:/app/ollama-init.sh:ro
    restart: unless-stopped
    networks:
      - llm_webui_network

    deploy:
      resources:
        reservations:
          devices:
            - driver: ${GPU_DRIVER:-}
              count: ${GPU_COUNT:-0}
              capabilities: ${GPU_CAPABILITIES:-[]}
    entrypoint: ["/bin/bash", "/app/ollama-init.sh"]

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "${WEBUI_PORT:-8080}:8080"
    volumes:
      - ../data/openwebui_data:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - llm_webui_network

networks:
  llm_webui_network:
    driver: bridge
