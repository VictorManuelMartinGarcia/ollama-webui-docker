
# Ollama WebUI Docker

Un sistema de chat local con modelos de lenguaje (LLM) completamente dockerizado, usando **Ollama** y **Open WebUI**. Detecci√≥n autom√°tica de hardware, descarga autom√°tica de modelos y gesti√≥n sencilla con scripts bash.

> **Completamente local** ‚Ä¢ Sin dependencias en la nube ‚Ä¢ Privacidad garantizada

---

## ‚ú® Caracter√≠sticas

- ‚úÖ **Detecci√≥n autom√°tica de hardware** - Detecta NVIDIA GPU, AMD ROCm o CPU autom√°ticamente
- ‚úÖ **Descarga autom√°tica de modelos** - Llama3, Mistral, Phi3 descargan solos al inicio
- ‚úÖ **Interfaz moderna** - Open WebUI con chat, historial y configuraci√≥n
- ‚úÖ **Persistencia de datos** - Los modelos e historial se mantienen entre reinicios
- ‚úÖ **Scripts de utilidad** - `start.sh` y `clean.sh` para gestionar f√°cilmente
- ‚úÖ **Completamente privado** - Todo corre localmente, sin conexiones externas
- ‚úÖ **F√°cil de migrar** - Copia la carpeta a otra m√°quina y funciona al instante

---

## üõ†Ô∏è Requisitos

| Recurso | M√≠nimo | Recomendado |
|---------|--------|-------------|
| **Espacio disco** | 25 GB | 50 GB+ |
| **RAM** | 8 GB | 16 GB+ |
| **GPU** | - | NVIDIA o AMD |
| **Software** | Docker + Docker Compose | √öltima versi√≥n |

**Instalar Docker:** [Instrucciones oficiales](https://docs.docker.com/get-docker/)

---

## üöÄ Inicio R√°pido

### Configuraci√≥n inicial (opcional)

Si quieres cambiar puertos o configuraci√≥n:

```bash
cp .env.example .env
# Edita .env con tus valores personalizados
nano .env
```

**Variables disponibles en `.env`:**
```env
# Puertos (por defecto 8080 para Open WebUI, 11434 para Ollama)
OLLAMA_PORT=11434
WEBUI_PORT=8080

# Configuraci√≥n de Ollama
OLLAMA_KEEP_ALIVE=5m          # Mantener modelo en memoria
OLLAMA_NUM_PARALLEL=1         # Peticiones paralelas
```

### Primer arranque (descargar√° modelos autom√°ticamente)

```bash
git clone https://github.com/tu-usuario/ollama_webui.git
cd ollama_webui
chmod +x scripts/start.sh scripts/clean.sh
scripts/start.sh
```

El script har√° todo autom√°ticamente:
1. üîç Detectar tu hardware (GPU o CPU)
2. üîß Generar `docker-compose.generated.yml` optimizado
3. üê≥ Arrancar Ollama y Open WebUI (en los puertos configurados en `.env`)
4. üì• Descargar modelos (llama3, mistral, phi3)
5. ‚úÖ Mostrar mensaje cuando est√© listo

**Tiempo estimado:** 20-60 minutos (primera vez, depende de tu conexi√≥n)

### Cuando veas "Todo listo ‚úÖ"

Abre en tu navegador (usa el puerto que configuraste en `.env`):
```
http://localhost:{WEBUI_PORT}
```

Selecciona un modelo y empieza a chatear. ¬°As√≠ de simple!

---

## üìÇ Estructura del Proyecto

```
ollama_webui/
‚îú‚îÄ‚îÄ scripts/                          # Scripts principales
‚îÇ   ‚îú‚îÄ‚îÄüöÄ start.sh                    # Arranca todo (hardware detection + modelos)
‚îÇ   ‚îî‚îÄ‚îÄüßπ clean.sh                    # Limpia contenedores + modelos + datos
‚îÇ
‚îú‚îÄ‚îÄ config/                           # ‚öôÔ∏è Configuraci√≥n de servicios
‚îÇ   ‚îî‚îÄ‚îÄüîß ollama-init.sh              # Inicializaci√≥n de Ollama (descarga modelos)
‚îÇ
‚îú‚îÄ‚îÄ compose/                          # üê≥ Docker Compose
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.template.yml   # Plantilla base (usa variables de .env)
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.generated.yml  # Se crea autom√°ticamente (no subir a Git)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                         # üíæ Datos persistentes (ignorar en Git)
‚îÇ   ‚îú‚îÄ‚îÄ ollama_data/                  # Modelos descargados
‚îÇ   ‚îî‚îÄ‚îÄ openwebui_data/               # üí¨ Historial de chat y configuraci√≥n
‚îÇ
‚îú‚îÄ‚îÄ üìñ README.md                      # Este archivo
‚îú‚îÄ‚îÄ ‚öñÔ∏è  LICENSE                        # MIT License
‚îú‚îÄ‚îÄ üìÅ .gitignore                     # Archivos ignorados en Git
‚îú‚îÄ‚îÄ ‚öôÔ∏è  .env.example                   # Ejemplo de configuraci√≥n (subir a Git)
‚îî‚îÄ‚îÄ üìÑ .env                           # Tu configuraci√≥n local (ignorar en Git)
```

---

## üìö Uso

### Comandos principales

```bash
# Primera vez - descarga modelos y arranca todo
./start.sh

# Parar contenedores (mantiene datos)
docker compose -f compose/docker-compose.generated.yml stop

# Reiniciar (r√°pido, sin descargar modelos)
docker compose -f compose/docker-compose.generated.yml start

# Limpiar todo (borra contenedores, modelos, datos)
./clean.sh
```

### Ver estado

```bash
# Estado de contenedores
docker compose -f compose/docker-compose.generated.yml ps

# Logs de Ollama
docker logs -f ollama

# Logs de Open WebUI
docker logs -f open-webui

# Listar modelos descargados
docker exec -it ollama ollama list
```

---

## ‚öôÔ∏è Configuraci√≥n

### Cambiar puertos

1. Copia el archivo de ejemplo:
   ```bash
   cp .env.example .env
   ```

2. Edita `.env` y cambia los puertos seg√∫n necesites:
   ```env
   OLLAMA_PORT=11434      # Cambiar puerto de Ollama si es necesario
   WEBUI_PORT=9000        # Cambiar puerto de Open WebUI (ej: 9000)
   ```

3. Ejecuta `./start.sh` para aplicar cambios:
   ```bash
   scripts/start.sh
   ```

4. Abre `http://localhost:9000` (o el puerto que configuraste)

### Optimizar Ollama

En `.env` puedes ajustar el comportamiento de Ollama:

```env
# Tiempo que Ollama mantiene el modelo cargado en memoria
OLLAMA_KEEP_ALIVE=5m     # Aumenta si quieres respuestas m√°s r√°pidas
                         # Disminuye si quieres liberar memoria

# Cu√°ntas peticiones procesa Ollama simult√°neamente  
OLLAMA_NUM_PARALLEL=1    # Aumenta si tienes mucha RAM/GPU
```

---

## üîß Personalizaci√≥n

### Cambiar modelos a descargar

Edita `ollama-init.sh` y modifica estas l√≠neas:

```bash
echo "[Ollama Init] Descargando modelos..."
echo "[Ollama Init] Descargando llama3..."
ollama pull llama3 2>&1 | while IFS= read -r line; do echo "[llama3] $line"; done

echo "[Ollama Init] Descargando mistral..."
ollama pull mistral 2>&1 | while IFS= read -r line; do echo "[mistral] $line"; done

echo "[Ollama Init] Descargando phi3..."
ollama pull phi3 2>&1 | while IFS= read -r line; do echo "[phi3] $line"; done
```

**Otros modelos disponibles en [ollama.com](https://ollama.com/library):**
- `neural-chat` - Chat optimizado
- `dolphin-mixtral` - Modelo potente
- `openchat` - Alternativa a Mistral
- `starling-lm` - Buen balance
- Y muchos m√°s...

Despu√©s de cambiar, ejecuta `./start.sh` para descargar los nuevos modelos.

### Configurar GPU manualmente

Si `start.sh` no detecta tu GPU correctamente:

1. Edita `docker-compose.template.yml` y busca la secci√≥n `deploy` en el servicio `ollama`

2. **Para NVIDIA:**
   ```yaml
   deploy:
     resources:
       reservations:
         devices:
           - driver: nvidia
             count: 1
             capabilities: [gpu]
   ```

3. **Para AMD ROCm:**
   ```yaml
   deploy:
     resources:
       reservations:
         devices:
           - driver: amd
             count: 1
             capabilities: [gpu]
   ```

4. Luego ejecuta:
   ```bash
   docker compose -f compose/docker-compose.generated.yml down
   ./start.sh
   ```

---

## üéØ Scripts disponibles

| Script | Funci√≥n | Tiempo |
|--------|---------|--------|
| `./start.sh` | Detecta hardware, genera Compose, arranca todo y descarga modelos | 20-60 min (1¬™ vez) |
| `./clean.sh` | Elimina contenedores, modelos y datos. Restaura estado inicial | 1-2 min |

---

## ‚ö° Rendimiento

### Velocidad por hardware

| Hardware | Velocidad aproximada |
|----------|---------------------|
| **CPU moderno** | 1-5 segundos/respuesta |
| **GPU NVIDIA** | 100-500ms/respuesta |
| **GPU AMD** | 100-500ms/respuesta |

### Optimizaciones

- **Aumentar RAM dedicada:** Edita Docker Desktop settings
- **Cerrar otras aplicaciones:** Libera memoria
- **Usar modelos m√°s peque√±os:** Phi3 es m√°s r√°pido que Llama3

---

## ‚ö†Ô∏è Consideraciones importantes

### Espacio en disco

Cada modelo ocupa:
- **llama3** ‚Üí 4.7 GB
- **mistral** ‚Üí 4.4 GB
- **phi3** ‚Üí 2.2 GB
- **Total por defecto** ‚Üí 11 GB

**Mant√©n m√≠nimo 25 GB libres** para no saturar el disco.

### Persistencia de datos

- `ollama_data/` ‚Üí Modelos descargados
- `openwebui_data/` ‚Üí Historial de chat y configuraci√≥n
- **Ambas se ignoran en Git** (ver `.gitignore`)
- **No las borres** si quieres mantener tus datos

### Migraci√≥n a otra m√°quina

```bash
# Copia todo incluyendo datos
cp -r ollama_webui /ruta/destino/

# En la m√°quina nueva
cd ollama_webui
./start.sh  # Continuar√° con modelos ya descargados
```

### Primera ejecuci√≥n

- La descarga de modelos **toma bastante tiempo** (20-60 min)
- El script mostrar√° progreso en tiempo real
- **No cierres el script** hasta que veas "Todo listo ‚úÖ"
- Puedes dejar corriendo mientras haces otras cosas

---

## üêõ Troubleshooting

### "docker: command not found"
‚Üí Instala Docker desde https://docs.docker.com/get-docker/

### El script se queda esperando
‚Üí Normal la primera vez. D√©jalo correr. Ver logs con `docker logs -f ollama`

### Open WebUI no muestra modelos
‚Üí Espera a que termine la descarga. Ver `docker exec -it ollama ollama list`

### Poco espacio en disco
‚Üí Ejecuta `./clean.sh` para limpiar y empezar de nuevo

---

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Para cambios importantes:

1. Fork el proyecto
2. Crea una rama (`git checkout -b feature/mejora`)
3. Commit cambios (`git commit -m 'A√±ade mejora'`)
4. Push a la rama (`git push origin feature/mejora`)
5. Abre un Pull Request

---

## üìù Licencia

Este proyecto est√° bajo licencia **MIT**. Ver [LICENSE](LICENSE) para detalles completos.

---
